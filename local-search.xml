<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>AI Fundamentals</title>
    <link href="/2023/01/14/AI-Fundamentals/"/>
    <url>/2023/01/14/AI-Fundamentals/</url>
    
    <content type="html"><![CDATA[<h2 id="c4-监督学习">C4 监督学习</h2><h3id="机器学习基础数据驱动学习data-driven-learning">机器学习基础：数据驱动学习(data-drivenlearning)</h3><h4 id="机器学习分类">机器学习分类：</h4><ul><li>监督学习(supervised learning)：有标签数据，回归或者分类</li><li>无监督学习(un-supervised learning)：无标签数据，聚类或者降维</li><li>强化学习(reinforcement learning) ：序列数据决策，与从环境交互</li></ul><h4 id="监督学习">监督学习</h4><p><strong>对象-标注数据：</strong>训练和测试数据：训练集，测试集。应用到未知数据集</p><p><strong>方法-学习模型</strong>： <strong>生成方法</strong>(generative approach) -生成模型(generative model) 学习联合概率分布𝑃（𝑋,𝑌） 方法：贝叶斯方法、隐马尔可夫链</p><p><strong>判别方法</strong> (discriminativeapproach)-判别模型(discriminative model) 学习判别函数𝒇(X)或者条件概率分布𝑷(Y|𝑿) 。 包括：回归模型、神经网络、支持向量机、Ada boosting</p><p><strong>评估-损失函数</strong></p><ul><li><strong>损失函数的类别</strong>：<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110213619005.png"alt="image-20230110213619005" /></li><li><strong>经验风险(empirical risk)</strong>：训练集中数据产生的损失，表征训练数据拟合程度。<strong>ERM</strong>：经验风险最小化<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110214217830.png"alt="image-20230110214217830" /></li><li><strong>期望风险(expected risk):</strong>当测试集中存在<strong>无穷多</strong>数据时产生的损失，表征学习所得模型效果。<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110214230474.png"alt="image-20230110214230474" /> <strong>注意</strong>样本容量趋于无穷时，经验风险趋于期望风险。但现实中训练样本数有限，要对经验风险进行一定的约束。</li><li><strong>过学习(over-fitting)</strong>：经验风险小，期望风险大<strong>欠学习(under-fitting)</strong>：经验风险、期望风险均大好的算法应该泛化能力强，经验风险、期望风险均小</li><li><strong>结构风险(structural riskminimization)最小化</strong>：引入正则化项 (regulatizer) 或惩罚项(penalty term ) <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110214544314.png"alt="image-20230110214544314" /></li></ul><h3 id="回归分析">回归分析</h3><h4 id="一元线性回归𝒚-𝒂𝒙-b">一元线性回归：𝒚 = 𝒂𝒙 + b</h4><p>方法：最小二乘法。可对𝑳(𝒂, 𝒃)参数𝒂和𝒃分别求导，令其导数值为零，再求取参数𝒂和𝒃的取值。 <span class="math display">\[𝑳（𝒂, 𝒃） = ∑^{n}_{𝒊=𝟏}（𝒚𝒊 -𝒂𝒙_𝒊 -𝒃 )^𝟐\]</span> <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110215729809.png"alt="image-20230110215729809" /></p><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110215741003.png"alt="image-20230110215741003" /><figcaption aria-hidden="true">image-20230110215741003</figcaption></figure><h4 id="多元线性回归𝒚-𝒂𝒙-a0">多元线性回归：𝒚 = 𝒂'·𝒙 + a0</h4><p><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110215935773.png"alt="image-20230110215935773" />最小化目标：<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110220000015.png"alt="image-20230110220000015" /></p><p>同样通过求导可得。<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110220204799.png"alt="image-20230110220204799" /></p><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110220226840.png"alt="image-20230110220226840" /><figcaption aria-hidden="true">image-20230110220226840</figcaption></figure><p>线性回归对离群点（outlier)敏感，导致模型建模不稳定，结果有偏。</p><h4 id="逻辑斯蒂回归对数几率回归">逻辑斯蒂回归/对数几率回归</h4><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110220940431.png"alt="image-20230110220940431" /><figcaption aria-hidden="true">image-20230110220940431</figcaption></figure><p>引入 <strong>sigmoid函数</strong>： <span class="math display">\[y = \frac{1}{1 + e^{-z}}\]</span></p><ul><li>单调递增，值域为(0，1)。输出可作为概率值。</li><li>对输入𝑧取值范围没有限制。但当𝑧大于（小于）一定数值后，函数输出无限趋近于1（0）。y(0)= 0.5</li></ul><p>用于<strong>二分类问题</strong>： y = 1表示输入数据𝒙属于正例，y =0表示输入数据𝒙属于负例。y理解为输入数据𝒙为正例概率，1-y理解为输入数据x为负例的概率。<strong>几率</strong>：p / (1-p)，反应相对可能性。从而定义<strong>对数几率/logit函数</strong><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110221621245.png"alt="image-20230110221621245" /></p><p><strong>判断</strong>（属于正例）：输入数据𝒙，属于正例的概率大于其属于负例的概率。即𝑝(𝑦=1|𝒙) &gt; 0.5。这等价于p(𝑦=1| 𝒙) / p(𝑦= 0| 𝒙) &gt; 1 ,即log(……) = 𝒘'𝒙 + b&gt; 0成立。是线性回归。</p><p><strong>参数w,b优化</strong> D表示观测（训练）数据，𝜃 = {𝒘,𝑏}表示模型参数。假设观测所得每一个样本数据是独立同分布(independent andidentically distributed, i.i.d）<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110222444238.png"alt="image-20230110222444238" /> 取对数有<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110222708321.png"alt="image-20230110222708321" /></p><p><strong>最大似然估计MLE</strong>目的是计算似然函数的最大值，而分类过程是需要损失函数最小化。因此在上式前加一个负号作为损失函数(交叉熵)<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110222844004.png"alt="image-20230110222844004" /> 求θ偏导，注意ℎθ' (𝑥) =ℎθ(𝑥)(1-ℎθ(𝑥))<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110223320783.png"alt="image-20230110223320783" /> 则梯度下降公式<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110223852330.png"alt="image-20230110223852330" /></p><p><strong>MLE和MAP</strong></p><h3 id="决策树">决策树</h3><p>通过树形结构来进行分类。可以看作是一系列以叶子节点为输出的决策规则（DecisionRules）.每个<strong>非叶子节点</strong>：对分类目标在某个属性上的判断，每个分支代表基于该属性做出的一个判断.每个<strong>叶子节点</strong>: 一 种分类结果</p><h4 id="信息熵">信息熵</h4><p>𝐾个信息组成了集合样本𝐷，记 第𝑘个信息发生的概率为𝑝k.则信息熵<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110224403302.png"alt="image-20230110224403302" />表征了𝑫的纯度。ED越小，纯度越高，𝑫包含的信息越确定。</p><p>好的决策树：划分属性的顺序选择重要。性能好的决策树随着划分不断进行，决策树分支结点样本集的“纯度”会越来越高，即其所包含样本尽可能属于相同类别。</p><h4 id="构建决策树">构建决策树</h4><ul><li>计算选择不同属性作为划分的指标。</li><li>选择最佳指标（最小）对原样本集进行划分。</li><li>重复以上操作直到划分后的不同子样本集都只存在同类样本。</li></ul><h4 id="指标的选择">指标的选择</h4><ul><li>信息增益：原来熵减新熵<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110225113563.png"alt="image-20230110225113563" /></li><li>信息增益率：纳入划分本身带来的信息info<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110225241718.png"alt="image-20230110225241718" /></li><li>Gini：从样本中选取样本同类的概率，更快。<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110225327991.png"alt="image-20230110225327991" /></li></ul><h3 id="线性区别分析lda-fda">线性区别分析LDA /FDA</h3><p>对于一组具有标签信息的<strong>高维数据</strong>样本，LDA利用类别信息将其线性投影到一个低维空间上，使得低维空间中<strong>同一类别样本尽可能靠近，不同类别样本尽可能彼此远离。</strong></p><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111000414904.png"alt="image-20230111000414904" /><figcaption aria-hidden="true">image-20230111000414904</figcaption></figure><h4 id="二分类">二分类</h4><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111000602809.png"alt="image-20230111000602809" /><figcaption aria-hidden="true">image-20230111000602809</figcaption></figure><p><strong>最小化𝑠1+𝑠2</strong>：投影后归属于同一类别的样本数据在投影后的空间中尽可能靠近。<strong>最大化|𝑚1 -𝑚2|^2 :</strong>投影后归属于两个类别的数据样本中心尽量远。 综上即最大化：<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111000902510.png"alt="image-20230111000902510" /></p><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111000939871.png"alt="image-20230111000939871" /><figcaption aria-hidden="true">image-20230111000939871</figcaption></figure><p>提取特征有<strong>类间散度矩阵(between-class scattermatrix)Sb</strong>： <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111001137035.png"alt="image-20230111001137035" /> 和<strong>类内散度矩阵(within-classscatter matrix)Sw</strong>:<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111001222326.png"alt="image-20230111001222326" /></p><p>由于𝐽 (𝒘) 的分子和分母都是关于𝒘的二项式，最后解只与𝒘的方向有关，与𝒘的长度无关. 令分母𝒘' Sw w =1，然后用拉格朗日乘子法来求解这个问题。</p><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111001356161.png"alt="image-20230111001356161" /><figcaption aria-hidden="true">image-20230111001356161</figcaption></figure><p>对𝒘求偏导并使其求导结果为零，结果有λ和w分别是特征根、特征向量。 <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111001506445.png"alt="image-20230111001506445" /> 令实数<strong>λ_{w}</strong> = (m2-m1)'w,则有<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111001955456.png"alt="image-20230111001955456" />把λ_{w}和λ放在w侧，取新的w约去（大小不影响)有：<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111002146677.png"alt="image-20230111002146677" /></p><h4 id="多分类">多分类</h4><p>优化目标：<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111002226258.png"alt="image-20230111002226258" /></p><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111002241780.png"alt="image-20230111002241780" /><figcaption aria-hidden="true">image-20230111002241780</figcaption></figure><h4 id="步骤">步骤</h4><ul><li>计算数据样本集中每个类别样本的均值</li><li>计算类内散度矩阵𝑺𝒘和类间散度矩阵𝑺𝒃</li><li>根据𝑺𝒘^{-1}𝑺𝒃𝑾 = λ𝑾 求解𝑺𝒘^{-1}Sb特征向量 𝒘𝟏, 𝒘𝟐,… , 𝒘𝒓，构成矩阵𝑾</li><li>通过矩阵𝑾将每个样本映射到低维空间，实现特征降维。</li></ul><h4 id="lda和pca">LDA和PCA</h4><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111002533542.png"alt="image-20230111002533542" /><figcaption aria-hidden="true">image-20230111002533542</figcaption></figure><h3 id="ada-boosting-自适应提升">Ada Boosting 自适应提升</h3><p>对于一个复杂的分类任务，可以将其分解为若干子任务，然后将若干子任务完成方法综合，最终完成该复杂任务。组合弱分类器(weak classifiers) 形成一个强分类器(strong classifier)。</p><h4 id="计算学习理论">计算学习理论</h4><p><strong>可计算</strong>：图灵可停机 <strong>霍夫丁不等式(Hoeffding’sinequality)</strong><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111003230720.png"alt="image-20230111003230720" /></p><p><strong>概率近似正确（PAC</strong>）</p><ul><li>研究范围： 假设正确/训练数据规模/假设空间复杂度/假设空间选择</li><li>概念：学习任务、实例集和概念集（对应）、假设空间、实例分布和数据集。</li><li>强可学习模型和弱可学习模型 <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111003704959.png"alt="image-20230111003704959" /></li><li>强可学习和弱可学习是等价的。</li></ul><h4 id="ada-boosting-的核心问题">Ada Boosting 的核心问题</h4><ul><li>如何在每个弱分类器学习过程中改变训练数据的权重：提高在上一轮中<strong>分类错误样本</strong>的权重。</li><li>如何将一系列弱分类器组合成强分类器：加权多数表决方法提高分类误差小的弱分类器的权重，减少分类误差大的弱分类器的权重。</li></ul><h4 id="算法描述">算法描述</h4><ol type="1"><li><p>数据样本权重（均等）初始化<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111004039001.png"alt="image-20230111004039001" /></p></li><li><p>分别训练第𝒎个弱分类器 - <strong>序列化学习</strong>机制</p><ul><li>使用具有分布权重𝑫𝒎的训练数据来学习得到第𝒎个基分类器（弱分类器）𝑮𝒎</li><li>计算𝑮𝒎 （𝒙） 在训练数据集上的加权分类误差<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111004222180.png"alt="image-20230111004222180" /></li><li>根据误差计算分类器权重 <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111004350565.png"alt="image-20230111004350565" /></li><li>更新样本分布权重 <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111004541929.png"alt="image-20230111004541929" /> 𝒁𝒎是归一化因子。<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111004612577.png"alt="image-20230111004612577" /></li></ul><p>在开始训练第𝒎+𝟏个弱分类器𝑮𝒎+𝟏之前对训练数据集中数据权重进行调整.</p></li><li><p>以线性加权形式来组合弱分类器𝒇(x)<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111005030201.png"alt="image-20230111005030201" /><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111005050153.png"alt="image-20230111005050153" /></p><p><strong>注意</strong> 𝜶𝒎累加之和并不等于1。</p></li></ol><h4 id="note">NOTE</h4><p><strong>霍夫丁不等式</strong><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111005346938.png"alt="image-20230111005346938" />所“组合”弱分类器越多，则学习分类误差呈指数级下降，直至为零。</p><p>前提条件：</p><ul><li>每个弱分类器产生的误差相互独立；</li><li>每个弱分 类器的误差率小于50%。</li></ul><p>每个弱分类器均是在同一个训练集上产生，条件1）难以满足。也就说，“准确性（对分类结果而言）”和“差异性（对每个弱分类器而言）”难以同时满足。</p><p><strong>本质</strong> ：最小化指数损失函数 <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111005732021.png"alt="image-20230111005732021" /></p><p><strong>分类误差上界</strong>：<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111005819218.png"alt="image-20230111005819218" /></p><p>在第𝒎次迭代中，AdaBoosting总是趋向于将具有最小误差的学习模型选做本轮生 成的弱分类器𝑮𝒎。</p><p><strong>回归与分类</strong></p><p>均是学习输入变量和输出变量之间潜在关系模型，基于学习所得模型将输入变量映射到输出变量。</p><p>回归分析：学习得到一个函数。将输入变量映射到<strong>连续输出</strong>空间，如价格和温度等，即值域是连续空间。</p><p>分类模型：学习得到一个函数将输入变量映射到<strong>离散输出</strong>空间，如人脸和汽车等，即值域是离散空间。</p>]]></content>
    
    
    
    <tags>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/01/13/hello-world/"/>
    <url>/2023/01/13/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
