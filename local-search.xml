<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>investigator&#39;s handbook Chapter1</title>
    <link href="/2023/01/29/COC-investigator-handbook/"/>
    <url>/2023/01/29/COC-investigator-handbook/</url>
    
    <content type="html"><![CDATA[<h2 id="introduction">Introduction</h2><p><em>Call of Cthulhu</em> is a horror <code>roleplaying game</code>based on the writings of <em>Howard Phillips Lovecraft</em>. Lovecraftpenned a tremendous body of work during the 1920s and 1930s concerningboth horrors from beyond and from within.</p><h3 id="purpose-of-the-game">Purpose of the Game</h3><p><strong><em>Players</em></strong> take the role of intrepidinvestigators of the unknown ("investigators"), attempting to seek out,understand and eventually destroy the horrors, mysteries, and secrets ofthe Cthulhu Mythos.</p><p><strong><em>A game moderator</em></strong>, known as the Keeper ofArcane Lore ("Keeper"), within the rules of the game, sets up situationsfor the players to confront.</p><p>The game rules <strong><em>use dice to determine</em></strong> if anaction succeeds or fails when a dramatic conflict presents itself.</p><h3 id="time-covered">Time Covered</h3><p>Many Call of Cthulhu scenarios are set in the United States in the1920s called the Classic Era. This book uses both the Classic Era andour own Modern-Day as period settings.</p><h3 id="preparations">Preparations</h3><ul><li>The Call of Cthulhu Rulebook—only needed by the Keeper.</li><li><strong><em>Roleplaying dice.</em></strong></li><li>Paper.</li><li>Pencils and an eraser.</li><li>Two or more people to game with—one person must play the role ofKeeper.</li><li>A quiet place.</li><li>Three or four hours in which to play the game.</li></ul><p>Players and Keeper should each have their own set of dice:</p><blockquote><ul><li>percentage dice (D100), consisting of two10-sided dice rolled at thesame time,0 - 9 and 00-90 respectively.</li><li>a four-sided die (D4)</li><li>a six-sided die (D6)</li><li>an eight-sided die (D8)</li><li>a twenty-sided die (D20).</li></ul></blockquote><h2 id="cooperation-and-competition">Cooperation and Competition</h2><blockquote><p>Gaming is a social pastime. Whether or not investigators cooperate,the players should.</p></blockquote><h3 id="winners-and-losers">Winners and Losers</h3><p>In COC, there are no winners and losers in the standard competitivesense.</p><ul><li><strong>common goal</strong>: to discover and foil some nefariousplot being perpetrated by the minions of some dark cult or secretsociety.</li><li><strong>opposition</strong>: alien or hostile controlled by animpartial Keeper.</li></ul><p>Winning in such a situation depends on <strong><em>whether theinvestigators succeed in their goal</em></strong>.</p><p><strong><em>Survived investigators</em></strong> will gain:</p><ul><li>power from arcane volumes of forgotten lore</li><li>knowledge of horrendous monsters</li><li>advancement in their skills (as they become more experienced! )</li></ul><p>The players’ investigators will continue to progress, un til theirdemise or retirement.</p><h2 id="section"></h2>]]></content>
    
    
    <categories>
      
      <category>COC</category>
      
      <category>Investigator&#39;s handbook</category>
      
    </categories>
    
    
    <tags>
      
      <tag>COC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>General Physics 25</title>
    <link href="/2023/01/26/GeneralPhysics-25/"/>
    <url>/2023/01/26/GeneralPhysics-25/</url>
    
    <content type="html"><![CDATA[<h2 id="charge">Charge</h2><h3 id="concept-and-properties">concept and properties</h3><ul><li><p>electric charge is attribute of body. Unlike charges attract,like charges repel.</p></li><li><p>charge is quantized. Any observed charge<strong><em>q</em></strong> turns out to be multiples of a certain<code>elementary charge</code> <strong><em>e</em></strong>. <spanclass="math display">\[e = 1.60217733 \times 10^{-19} C \\q = 0, \pm e, \pm2e,\pm3e \cdots\]</span></p></li></ul><h3 id="notes"><code>Notes</code></h3><ol type="1"><li>Two <code>elementary charged particles</code> inside atoms,<code>proton</code> and <code>electron</code>, make chargequantized.</li><li><strong><em>e</em></strong> is very small and of no sign.</li><li>Neutrons and Protons consist of quarks( <spanclass="math inline">\(-\frac{1}{3}e\)</span> or <spanclass="math inline">\(\frac{2}{3}e\)</span>).</li></ol><h3 id="applications">Applications</h3><p>Electrostatic paint spraying, Powder coating, Fly-ash precipitation,Nonimpact ink-jet printing and Photocopying.</p><h2 id="coulombs-law1785">Coulomb's Law,1785</h2><blockquote><p>The repulsive force between two <strong><em>smallspheres</em></strong> charged with same sort of electricity is in theinverse ratio of the squares of the distances between the centers of thespheres. <span class="math display">\[F \propto{\frac{q_1q_2}{r^2}}\]</span></p></blockquote><p>The force from 1 acting on 2 is: <span class="math display">\[\vec{F}_{12} = \frac{kq_1q_2}{r_{12}^2}\hat{r_{12}} =\frac{1}{4\pi\epsilon_0}\frac{q_1q_2}{r_{12}^2}\hat{r_{12}}\]</span></p><h3 id="units">Units</h3><table style="width:100%;"><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">vector / scalar</th><th style="text-align: center;">unit</th><th style="text-align: center;">notes</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(r_{12}\)</span></td><td style="text-align: center;">scalar</td><td style="text-align: center;">meter</td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\hat{r_{12}}\)</span></td><td style="text-align: center;">vector</td><td style="text-align: center;">1</td><td style="text-align: center;">unit vector pointing from 1 to 2</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\vec{F_{12}}\)</span></td><td style="text-align: center;">vector</td><td style="text-align: center;">Newton</td><td style="text-align: center;">same as <spanclass="math inline">\(\hat{r_{12}}\)</span></td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(q\)</span></td><td style="text-align: center;">scalar</td><td style="text-align: center;">Coulomb</td><td style="text-align: center;">signed positive/negative</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(k\)</span></td><td style="text-align: center;">scalar</td><td style="text-align: center;"><span class="math inline">\(N\cdot m^2/C^2\)</span></td><td style="text-align: center;">a special constant, <spanclass="math inline">\(k = 9 \times 10^9N\cdot m^2 /C^2\)</span></td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\epsilon_0\)</span></td><td style="text-align: center;">scalar</td><td style="text-align: center;"><span class="math inline">\(C^2/(N\cdotm^2)\)</span></td><td style="text-align: center;">an elementary constant, <spanclass="math inline">\(\epsilon_0 = 8.854 \times 10^{-12}C^2/(N\cdotm^2)\)</span></td></tr></tbody></table><h3 id="notes-1"><code>Notes</code></h3><ol type="1"><li><p>Coulomb's Law generally holds <strong><em>only for</em></strong>charged objects whose <strong><em>size are much smaller than thedistance</em></strong> between them. <spanclass="math inline">\(i.e.\)</span> <strong><em>only for pointcharges</em></strong></p></li><li><p>Assume that <span class="math inline">\(F =k\frac{q_1q_2}{r^\lambda}\)</span>, experiments show that <spanclass="math inline">\(\lambda = 2.01\)</span>(by Torsion balance) and<span class="math inline">\(\lambda = 2 \pm 10^{-6}\)</span>(by indirectexperiment).</p></li><li><p>Law of gravitation, <span class="math inline">\(\vec{F} =m\vec{a}\)</span> , defines <code>inertial mass</code><spanclass="math inline">\(m\)</span> and determines<code>gravity constant</code>G. Coulomb's law defines k first, then thebasic unit of charge is determined. <span class="math inline">\(1 C = 6\times 10^{18}e\)</span>.</p></li><li><p>Newton's law of gravitation could be considered an<em>approximation</em> of theory of relativity, while Coulomb's law isan <strong><em>exact result</em></strong> (not an approximation fromhigher laws) and remains <strong><em>valid in quantumlimit</em></strong>.</p></li><li><p>At level of electrons, Electric force is much stronger thangravitational force. <span class="math display">\[\frac{F_{elec}}{F_{grav}} = \frac{q_1q_2}{m_1m_2}\frac{k}{G} = 4.17\times 10^{42}\]</span> For an electron, <span class="math display">\[q = 1.6 \times 10^{-19} C\\m_e = 9.1 \times 10^{-31} kg\]</span></p></li></ol><h2 id="continuous-charge-distribution">Continuous ChargeDistribution</h2><p>For discrete charge distribution, the superpositions of vectors maywork.</p><h2 id="vecs-and-scals">Vecs and Scals</h2><h3 id="notation">Notation</h3><p>Vectors are specified by <strong><em>magnitude(length)</em></strong>and <strong><em>direction</em></strong>, and written like <spanclass="math inline">\(\vec{F},\vec{v},\hat{r}\)</span>.<br />The <strong><em>magnitude</em></strong> is a scalar quantity, <spanclass="math inline">\(\vert\vec{F}\vert = F\)</span> . The<code>unit vector</code> is denoted by <code>^</code>, indicating<strong><em>only a direction</em></strong> (it has no units!). <spanclass="math inline">\(\hat{r} = \frac{\vec{r}}{\vert\vec{r}\vert}\)</span></p><p><strong>Composition:</strong> Vectors can be composited into <spanclass="math inline">\(x,y\)</span> and <spanclass="math inline">\(z\)</span> components. <spanclass="math inline">\(\vec{F} = F_x\hat{x} + F_y\hat{y} +F_z\hat{z}\)</span> .</p><h3 id="superposition">Superposition</h3><p><span class="math display">\[\vec{F} = \sum{\vec{F_i}}\]</span></p><h2 id="conductors-insulators">Conductors, Insulators …</h2><table style="width:100%;"><thead><tr class="header"><th></th><th>Once charged, the charges</th><th></th></tr></thead><tbody><tr class="odd"><td>Insulators</td><td>Not free to move (averagely <span class="math inline">\(\leq1e/cm^3\)</span>)</td><td>Glass, Plastic, Dry wood</td></tr><tr class="even"><td>Semiconductors</td><td>(averagely <span class="math inline">\(10^{10} \sim10^{12}e/cm^3\)</span>)</td><td>Silicon, Germanium</td></tr><tr class="odd"><td>Conductors</td><td>All free to move ((averagely $ 10<sup>{23}e/cm</sup>3$)</td><td>Aluminum, Copper, Iron, Silver</td></tr><tr class="even"><td>Superconductors</td><td>R = 0,B = 0 under some pressure and temperature conditions</td><td></td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>General Physics</category>
      
      <category>E &amp; M</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Physics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI Fundamentals - C1 绪论</title>
    <link href="/2023/01/15/AI-Fundamentals-1/"/>
    <url>/2023/01/15/AI-Fundamentals-1/</url>
    
    <content type="html"><![CDATA[<h3 id="人工智能历史">人工智能历史</h3><ul><li>起源：1955，《人工智能达特茅斯夏季研究项目提案》</li><li>本书定义：以机器为载体所实现的人类/生物智能。</li></ul><h3 id="可计算载体">可计算载体</h3><h4 id="形式化系统">形式化系统</h4><p>以下性质保证一个形式化系统的有效性：</p><blockquote><p><code>完备性</code>：所有能由某一形式化系统推得的知识可由该系统推得。<code>一致性/自洽性self-consistency/非矛盾性</code>：所有该系统可推导的知识不同时能推得其否定。<code>可判定性</code>：所有该系统可推得的知识，存在一个算法可在有限步内判定其真假。</p></blockquote><p><code>哥德尔不完备定理</code>任何表达力足够强的（递归可枚举）形式系统不同时具有一致性和完备性。系统本身的一致性不能在系统内证明。</p><h4 id="机械化图灵机">机械化：图灵机</h4><p><strong>描述</strong>：图灵机是一个抽象的机械计算装置，它有一条无限长的纸带，纸带分成了一个一个可擦写小方格，可放入数据、指令或保持空置。程序事先储存指令集，读写控制指针从左到右依次从纸带上读入信息并按照指令集进行操作，输出结果到纸带方格上，直到计算结束。即图灵机停机。</p><p><strong>可计算</strong>：一个任务是可计算的，当其对应的图灵机可在有限步骤后停机。</p><p><strong>注意</strong><code>原始递归函数</code>和<code>λ-演算</code>和<code>图灵机</code>在功能上等效。任何可计算函数（/不可计算函数）通过这三种方式均可（/不可）完成计算。</p><h3 id="智能计算方法">智能计算方法</h3><blockquote><p>推理、搜索和约束满足是人工智能求解的三大方法。</p></blockquote><h4 id="符号主义为核心的逻辑推理">符号主义为核心的逻辑推理</h4><p>推理是从已知前提推出新结论的过程。符号主义的人工智能的推理建立在一套高度概括抽象、严格精确的符号系统中。</p><ul><li><p>归纳推理：从特殊到一般</p></li><li><p>演绎推理：从一般到特殊</p></li><li><p>因果推理：判断事物间存在的因果关系。常用方法：结构因果模型SCM和因果图<em>casual diagram</em>。</p><ul><li><p>关联<em>association</em>： 统计相关</p></li><li><p>干预<em>intervention</em> ： 无法直接从数据得到的关系</p></li><li><p>反事实<em>counterfactual</em> ：条件取反向值观测到的结果。用来定义因果关系。</p><blockquote><p>条件变量A，观测到结果B。令A取负向值，观测到结果B'。A和B存在因果关系，当且仅当B和B'差异存在，且统计上显著。即因为A，所以B；如果没有A，那么没有B。A是B的唯一前提。</p></blockquote></li></ul></li></ul><p>应用： 专家系统</p><h4 id="问题求解为核心的搜索">问题求解为核心的搜索</h4><p>搜索是根据已有信息寻找满足约束条件的问题答案。</p><ul><li>无信息搜索<em>uninformed</em>: DFS,BFS</li><li>有信息搜索<em>informed</em>: 贪婪最佳优先搜索，A star搜索</li><li>对抗搜索<em>adversarial/ game</em>：最小最大，ab剪枝，蒙特卡洛树搜索是通过两个智能体通过竞争实现相反的利益实现的。</li></ul><h4 id="数据驱动为核心的机器学习">数据驱动为核心的机器学习</h4><h4 id="博弈对抗为核心的决策智能">博弈对抗为核心的决策智能</h4>]]></content>
    
    
    <categories>
      
      <category>AI Fundamentals</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI Fundamentals - C4 监督学习</title>
    <link href="/2023/01/14/AI-Fundamentals/"/>
    <url>/2023/01/14/AI-Fundamentals/</url>
    
    <content type="html"><![CDATA[<h3id="机器学习基础数据驱动学习data-driven-learning">机器学习基础：数据驱动学习(data-drivenlearning)</h3><h4 id="机器学习分类">机器学习分类：</h4><ul><li>监督学习(supervised learning)：有标签数据，回归或者分类</li><li>无监督学习(un-supervised learning)：无标签数据，聚类或者降维</li><li>强化学习(reinforcement learning) ：序列数据决策，与从环境交互</li></ul><h4 id="监督学习">监督学习</h4><p>监督学习的三个要素是对象、方法和评估。</p><p><strong>对象-标注数据：</strong>训练和测试数据：训练集，测试集。应用到未知数据集</p><p><strong>方法-学习模型</strong>： <strong>生成方法</strong>(generative approach) -生成模型(generative model) 学习联合概率分布𝑃（𝑋,𝑌） 方法：贝叶斯方法、隐马尔可夫链</p><p><strong>判别方法</strong> (discriminativeapproach)-判别模型(discriminative model) 学习判别函数𝒇(X)或者条件概率分布𝑷(Y|𝑿) 。 包括：回归模型、神经网络、支持向量机、Ada boosting</p><p><strong>评估-损失函数</strong></p><ul><li><p><strong>常用损失函数</strong>：</p><p><img src="/img/posts/AI/4_1.jpg" /></p></li><li><p><strong>经验风险(empirical risk)</strong>：训练集中数据产生的损失，表征训练数据拟合程度。<strong>ERM</strong>：经验风险最小化 <span class="math display">\[\min_{f \in \Phi}{\frac{1}{n}\sum_{i = 1}^{n}{Loss(y_i,f(x_i))}}\]</span></p></li><li><p><strong>期望风险(expected risk):</strong>当测试集中存在<strong>无穷多</strong>数据时产生的损失，表征学习所得模型效果。<span class="math display">\[\min_{f \in \Phi}{\int_{x \times y}{Loss(y,f(x))P(x,y) dxdy}}\]</span> <strong>注意</strong>样本容量趋于无穷时，经验风险趋于期望风险。但现实中训练样本数有限，要对经验风险进行一定的约束。</p></li><li><p><strong>过学习(over-fitting)</strong>：经验风险小，期望风险大<strong>欠学习(under-fitting)</strong>：经验风险、期望风险均大好的算法应该泛化能力强，经验风险、期望风险均小</p></li><li><p><strong>结构风险(structural riskminimization)最小化</strong>：引入正则化项 (regulatizer) 或惩罚项(penalty term ) <span class="math display">\[\min_{f \in \Phi}{(\frac{1}{n} \sum_{i = 1}^{n}{Loss(y_i,f(x_i))} +\lambda J(f))}\]</span></p></li></ul><h3 id="回归分析">回归分析</h3><h4 id="一元线性回归𝒚-𝒂𝒙-b">一元线性回归：𝒚 = 𝒂𝒙 + <em>b</em></h4><p>方法：最小二乘法。可对𝑳(𝒂, 𝒃)参数𝒂和𝒃分别求导，令其导数值为零，再求取参数𝒂和𝒃的取值。 <span class="math display">\[𝑳（𝒂, 𝒃） = ∑^{n}_{𝒊=𝟏}（𝒚𝒊 -𝒂𝒙_𝒊 -𝒃 )^𝟐\]</span> <span class="math display">\[a = \frac{\sum_{i = 1}^{n}{x_iy_i} - n\bar{x}\bar{y}}{\sum_{i =1}^{n}x_i^2 - n\bar{x}^2},\quad b = \bar{y} - a\bar{x}\]</span></p><h4 id="多元线性回归𝒚-𝒂𝒙-a0">多元线性回归：𝒚 = 𝒂'·𝒙 + a0</h4><p><span class="math display">\[f(x_i) = a_0 + \sum_{j = 1}^{D}{a_jx_{i,j}} = a_0 +\vec{a}^\tau\vec{x}_i\]</span></p><p>最小化目标： <span class="math display">\[J_m = \frac{1}{m} \sum_{i = 1}^{m}{((y_i) - f(\vec{x}_i))^2}\]</span> 同样通过求导可令 <span class="math display">\[\nabla{J(a)} = -2X(y - X^\tau\vec{a}) = 0\]</span> 即有 <span class="math display">\[XX^\tau\vec{a} = X\vec{y}\]</span></p><p><span class="math display">\[\vec{a} = (XX^\tau)^{-1}X\vec{y}\]</span></p><p>线性回归对离群点（outlier)敏感，离群点会导致模型建模不稳定，结果有偏。</p><h4 id="逻辑斯蒂回归对数几率回归">逻辑斯蒂回归/对数几率回归</h4><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110220940431.png"alt="image-20230110220940431" /><figcaption aria-hidden="true">image-20230110220940431</figcaption></figure><p>引入 <strong>sigmoid函数</strong>： <span class="math display">\[y = \frac{1}{1 + e^{-z}}\]</span></p><ul><li>单调递增，值域为(0，1)。输出可作为概率值。</li><li>对输入𝑧取值范围没有限制。但当𝑧大于（小于）一定数值后，函数输出无限趋近于1（0）。y(0)= 0.5</li></ul><p>用于<strong>二分类问题</strong>： y = 1表示输入数据𝒙属于正例，y =0表示输入数据𝒙属于负例。y理解为输入数据𝒙为正例概率，1-y理解为输入数据x为负例的概率。<strong>几率</strong>：p / (1-p)，反应相对可能性。从而定义<strong>对数几率/logit函数</strong><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110221621245.png"alt="image-20230110221621245" /></p><p><strong>判断</strong>（属于正例）：输入数据𝒙，属于正例的概率大于其属于负例的概率。即𝑝(𝑦=1|𝒙) &gt; 0.5。这等价于<span class="math inline">\(\frac{p(𝑦=1| 𝒙)}{p(𝑦=0| 𝒙)} &gt; 1\)</span> ,即log(……) = 𝒘'𝒙 + b &gt; 0成立。是线性回归。</p><p><strong>参数w,b优化</strong> D表示观测（训练）数据，𝜃 = {𝒘,𝑏}表示模型参数。假设观测所得每一个样本数据是独立同分布(independent andidentically distributed, i.i.d）<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110222444238.png"alt="image-20230110222444238" /> 取对数有<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110222708321.png"alt="image-20230110222708321" /></p><p><strong>最大似然估计MLE</strong>目的是计算似然函数的最大值，而分类过程是需要损失函数最小化。因此在上式前加一个负号作为损失函数(交叉熵)<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110222844004.png"alt="image-20230110222844004" /> 求θ偏导，注意ℎθ' (𝑥) =ℎθ(𝑥)(1-ℎθ(𝑥))<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110223320783.png"alt="image-20230110223320783" /> 则梯度下降公式<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110223852330.png"alt="image-20230110223852330" /></p><p><strong>MLE和MAP</strong></p><h3 id="决策树">决策树</h3><p>通过树形结构来进行分类。可以看作是一系列以叶子节点为输出的决策规则（DecisionRules）.每个<strong>非叶子节点</strong>：对分类目标在某个属性上的判断，每个分支代表基于该属性做出的一个判断.每个<strong>叶子节点</strong>: 一 种分类结果</p><h4 id="信息熵">信息熵</h4><p>𝐾个信息组成了集合样本𝐷，记 第𝑘个信息发生的概率为𝑝k.则信息熵<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110224403302.png"alt="image-20230110224403302" />表征了𝑫的纯度。ED越小，纯度越高，𝑫包含的信息越确定。</p><p>好的决策树：划分属性的顺序选择重要。性能好的决策树随着划分不断进行，决策树分支结点样本集的“纯度”会越来越高，即其所包含样本尽可能属于相同类别。</p><h4 id="构建决策树">构建决策树</h4><ul><li>计算选择不同属性作为划分的指标。</li><li>选择最佳指标（最小）对原样本集进行划分。</li><li>重复以上操作直到划分后的不同子样本集都只存在同类样本。</li></ul><h4 id="指标的选择">指标的选择</h4><ul><li>信息增益：原来熵减新熵<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110225113563.png"alt="image-20230110225113563" /></li><li>信息增益率：纳入划分本身带来的信息info<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110225241718.png"alt="image-20230110225241718" /></li><li>Gini：从样本中选取样本同类的概率，更快。<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230110225327991.png"alt="image-20230110225327991" /></li></ul><h3 id="线性区别分析lda-fda">线性区别分析LDA /FDA</h3><p>对于一组具有标签信息的<strong>高维数据</strong>样本，LDA利用类别信息将其线性投影到一个低维空间上，使得低维空间中<strong>同一类别样本尽可能靠近，不同类别样本尽可能彼此远离。</strong></p><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111000414904.png"alt="image-20230111000414904" /><figcaption aria-hidden="true">image-20230111000414904</figcaption></figure><h4 id="二分类">二分类</h4><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111000602809.png"alt="image-20230111000602809" /><figcaption aria-hidden="true">image-20230111000602809</figcaption></figure><p><strong>最小化𝑠1+𝑠2</strong>：投影后归属于同一类别的样本数据在投影后的空间中尽可能靠近。<strong>最大化|𝑚1 -𝑚2|^2 :</strong>投影后归属于两个类别的数据样本中心尽量远。 综上即最大化：<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111000902510.png"alt="image-20230111000902510" /></p><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111000939871.png"alt="image-20230111000939871" /><figcaption aria-hidden="true">image-20230111000939871</figcaption></figure><p>提取特征有<strong>类间散度矩阵(between-class scattermatrix)Sb</strong>： <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111001137035.png"alt="image-20230111001137035" /> 和<strong>类内散度矩阵(within-classscatter matrix)Sw</strong>:<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111001222326.png"alt="image-20230111001222326" /></p><p>由于𝐽 (𝒘) 的分子和分母都是关于𝒘的二项式，最后解只与𝒘的方向有关，与𝒘的长度无关. 令分母𝒘' Sw w =1，然后用拉格朗日乘子法来求解这个问题。</p><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111001356161.png"alt="image-20230111001356161" /><figcaption aria-hidden="true">image-20230111001356161</figcaption></figure><p>对𝒘求偏导并使其求导结果为零，结果有λ和w分别是特征根、特征向量。 <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111001506445.png"alt="image-20230111001506445" /> 令实数<strong>λ_{w}</strong> = (m2-m1)'w,则有<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111001955456.png"alt="image-20230111001955456" />把λ_{w}和λ放在w侧，取新的w约去（大小不影响)有：<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111002146677.png"alt="image-20230111002146677" /></p><h4 id="多分类">多分类</h4><p>优化目标：<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111002226258.png"alt="image-20230111002226258" /></p><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111002241780.png"alt="image-20230111002241780" /><figcaption aria-hidden="true">image-20230111002241780</figcaption></figure><h4 id="步骤">步骤</h4><ul><li>计算数据样本集中每个类别样本的均值</li><li>计算类内散度矩阵𝑺𝒘和类间散度矩阵𝑺𝒃</li><li>根据𝑺𝒘^{-1}𝑺𝒃𝑾 = λ𝑾 求解𝑺𝒘^{-1}Sb特征向量 𝒘𝟏, 𝒘𝟐,… , 𝒘𝒓，构成矩阵𝑾</li><li>通过矩阵𝑾将每个样本映射到低维空间，实现特征降维。</li></ul><h4 id="lda和pca">LDA和PCA</h4><figure><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111002533542.png"alt="image-20230111002533542" /><figcaption aria-hidden="true">image-20230111002533542</figcaption></figure><h3 id="ada-boosting-自适应提升">Ada Boosting 自适应提升</h3><p>对于一个复杂的分类任务，可以将其分解为若干子任务，然后将若干子任务完成方法综合，最终完成该复杂任务。组合弱分类器(weak classifiers) 形成一个强分类器(strong classifier)。</p><h4 id="计算学习理论">计算学习理论</h4><p><strong>可计算</strong>：图灵可停机 <strong>霍夫丁不等式(Hoeffding’sinequality)</strong><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111003230720.png"alt="image-20230111003230720" /></p><p><strong>概率近似正确（PAC</strong>）</p><ul><li>研究范围： 假设正确/训练数据规模/假设空间复杂度/假设空间选择</li><li>概念：学习任务、实例集和概念集（对应）、假设空间、实例分布和数据集。</li><li>强可学习模型和弱可学习模型 <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111003704959.png"alt="image-20230111003704959" /></li><li>强可学习和弱可学习是等价的。</li></ul><h4 id="ada-boosting-的核心问题">Ada Boosting 的核心问题</h4><ul><li>如何在每个弱分类器学习过程中改变训练数据的权重：提高在上一轮中<strong>分类错误样本</strong>的权重。</li><li>如何将一系列弱分类器组合成强分类器：加权多数表决方法提高分类误差小的弱分类器的权重，减少分类误差大的弱分类器的权重。</li></ul><h4 id="算法描述">算法描述</h4><ol type="1"><li><p>数据样本权重（均等）初始化<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111004039001.png"alt="image-20230111004039001" /></p></li><li><p>分别训练第𝒎个弱分类器 - <strong>序列化学习</strong>机制</p><ul><li>使用具有分布权重𝑫𝒎的训练数据来学习得到第𝒎个基分类器（弱分类器）𝑮𝒎</li><li>计算𝑮𝒎 （𝒙） 在训练数据集上的加权分类误差<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111004222180.png"alt="image-20230111004222180" /></li><li>根据误差计算分类器权重 <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111004350565.png"alt="image-20230111004350565" /></li><li>更新样本分布权重 <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111004541929.png"alt="image-20230111004541929" /> 𝒁𝒎是归一化因子。<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111004612577.png"alt="image-20230111004612577" /></li></ul><p>在开始训练第𝒎+𝟏个弱分类器𝑮𝒎+𝟏之前对训练数据集中数据权重进行调整.</p></li><li><p>以线性加权形式来组合弱分类器𝒇(x)<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111005030201.png"alt="image-20230111005030201" /><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111005050153.png"alt="image-20230111005050153" /></p><p><strong>注意</strong> 𝜶𝒎累加之和并不等于1。</p></li></ol><h4 id="note">NOTE</h4><p><strong>霍夫丁不等式</strong><imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111005346938.png"alt="image-20230111005346938" />所“组合”弱分类器越多，则学习分类误差呈指数级下降，直至为零。</p><p>前提条件：</p><ul><li>每个弱分类器产生的误差相互独立；</li><li>每个弱分 类器的误差率小于50%。</li></ul><p>每个弱分类器均是在同一个训练集上产生，条件1）难以满足。也就说，“准确性（对分类结果而言）”和“差异性（对每个弱分类器而言）”难以同时满足。</p><p><strong>本质</strong> ：最小化指数损失函数 <imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111005732021.png"alt="image-20230111005732021" /></p><p><strong>分类误差上界</strong>：<imgsrc="C:\Users\86173\AppData\Roaming\Typora\typora-user-images\image-20230111005819218.png"alt="image-20230111005819218" /></p><p>在第𝒎次迭代中，AdaBoosting总是趋向于将具有最小误差的学习模型选做本轮生 成的弱分类器𝑮𝒎。</p><p><strong>回归与分类</strong></p><p>均是学习输入变量和输出变量之间潜在关系模型，基于学习所得模型将输入变量映射到输出变量。</p><p>回归分析：学习得到一个函数。将输入变量映射到<strong>连续输出</strong>空间，如价格和温度等，即值域是连续空间。</p><p>分类模型：学习得到一个函数将输入变量映射到<strong>离散输出</strong>空间，如人脸和汽车等，即值域是离散空间。</p>]]></content>
    
    
    <categories>
      
      <category>AI Fundamentals</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/01/13/hello-world/"/>
    <url>/2023/01/13/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
